В процессе спринта был реализован stg слой, в который из источников была загрузка данных as is.

### Шаг 0 - Проверка готовности шаблонов.
Раз я пишу коммента значит репо скопировал с шаблонами, но что важнее необходимая задача это проверить что
stg слой заполняется корректно в postgres и топике Kafka,
доказывая тем самым, что сервис, написанный в спринте, работает корректно.
Данные есть в каждом из вышеприведенных стоков, а сервис стабильно работает.

### Шаг 1 - Создание второго сервиса: распланировать работу над сервисом.
Так как второй сервис это dds слой потока данных, то именно этот этап будет самым объемным по количеству тасков.
Так как для каждой таблицы в dds схеме свой запрос будет реализован. И основная сложность этого шага в количестве
требуемых скриптов и впринципе реализации сервиса без инструкций. Также необходимо спроектировать стоки для Kafka
где будут лежать данные для витрин слоя CDM. Буду описывать все по ходу реализации

### Шаг 2 - Создание второго сервиса: написать код сервиса.
1) Необходимо доработать файлы dds_message_processor_job.py, app.py, app_config.py и добавить новый файл .env с
переменными, в app необходимо прописать процесс с вызовом из файла dds_message_processor_job: class DdsMessageProcessor.
Для начала реализуем сразу выборку данных "только заказы": object_type=='order' и отфильтровонная выборка
будет поступать в продьюсер dds-service-orders, в том же формате и структуре.
2) После теста работы чтения и записи в топик через docker контейнер, необходимо реализовать запись данных в postgres.
Во все якоря, линки и сателиты.


